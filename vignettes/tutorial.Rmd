---
title: "Project 2: STAT302package Tutorial"
author: "Caiwei Tian"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{STAT302package Tutorial}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```


# Introduction
This package is primarily used for data analysis. It includes four functions that enable computation of t-test, linear model fitting, k nearest neighbor cross-validation and random forest cross-validation.

Use the following command to install the STAT302package.

```{r,eval=FALSE}
devtools::install_github("caiwet/STAT302package")
```

To begin, load our package first.
```{r setup}
library(STAT302package)
library(ggplot2)
library(knitr)
```

# Tutorial for my_t.test
Use lifeExp data from my_gapminder.
```{r}
data <- my_gapminder$lifeExp
```

Test 1
```{r}
my_t.test(data, "two.sided", 60)
```

The p-value is greater than 0.05, so the probability of observing a result as or more extreme than what we observed, assuming the null hypothesis is true is greater than the p-value cut-off. Thus, we don't have sufficient evidence to reject the null hypothesis that the true mean of life expectancy is equal to 60.

Test 2
```{r}
my_t.test(data, "less", 60)
```

This time the p-value is less than 0.05. We can conclude that our results are statistically significant. We are confident to reject null hypothesis (life expectancy = 60).

Test 3
```{r}
my_t.test(data, "greater", 60)
```

The p-value is much larger than 0.05. We do not have sufficient evidence to reject null hypothesis (life expectancy = 60).

# Tutorial for my_lm

```{r}
# Fit a linear model using lifeExp as response variable and gdpPercap and 
# continent as explanatory variables
lifeExp <- my_gapminder$lifeExp
gdpPercap <- my_gapminder$gdpPercap
continent <- my_gapminder$continent
input <- data.frame("lifeExp" = lifeExp, "gdpPercap" = gdpPercap, 
                    "continent" = continent)
output <- my_lm(lifeExp ~ gdpPercap + continent, input)
print(output)
```

Here we see that the coefficient for gdpPercap is about 4.789. That is to say, if gdpPercap increases by one unit, the mean life expectancy will increase by 4.789. 

The hypothesis test associated with the gdpPercap coefficient: 
null hypothesis is gdpPercap coefficient = 0 (there's no relationship between gdpPercap and lifeExp) and
alternative hypothesis is gdpPercap coefficient is not equal to 0 (two sided test).

The p_value we got for this hypothesis test also can be read from the table above. It is very very small and definitely less than alpha = 0.05. Thus, we have sufficient evidence to support that the null hypothesis is true and there is a relationship between gdpPercap and lifeExp.

```{r,fig.width=6}
# plot the actual vs. fitted values
my_coef <- output$Estimate
my_matrix <- model.matrix(lifeExp ~ gdpPercap + continent, data = my_gapminder)

y_hat <- my_matrix %*% as.matrix(my_coef)
my_df <- data.frame(actual = my_gapminder$lifeExp, fitted = y_hat, 
                    continent = my_gapminder$continent)

ggplot(my_df, aes(x = lifeExp, y = fitted)) +
  geom_point(aes(colour = factor(continent))) +
  geom_abline(slope = 1, intercept = 0, col = "black", lty = 2) +
  theme_bw(base_size = 15) +
  labs(x = "Fitted values", y = "Actual values", title = "Actual vs. Fitted") +
  theme(plot.title = element_text(hjust = 0.5))
```

If the model fits well, we would see all points around the line. In our model, we can see some points fitted well in the Europe continent set and when fitted values are high (60 - 80). However, in general, we generate many different predictions for different data points even though their actual values are very similar. There are also points in the continent = Asia set being very far away from the line. I think this indicates that our independent variable is not enough. We missed information when predicting. 


# Tutorial for my_knn_cv

```{r}
penguins_df <- na.omit(my_penguins)
penguins_cl <- penguins_df$species
penguins_df <- penguins_df %>%
  dplyr::select(bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g)

k_nn <- (1:10)
training_miss <- c()
cv_miss <- c()
for (i in 1:10) {
  predict <- my_knn_cv(penguins_df, penguins_cl, k_nn[i], 5)
  cv_miss <- c(cv_miss, predict$cv_error)
  pred <- predict$class
  err <- 0
  for (i in 1:length(penguins_cl)) {
    if (penguins_cl[i] != pred[i]) {
      err <- err + 1
    }
  }
  err <- err / length(penguins_cl)
  training_miss <- c(training_miss, err)
}
print(training_miss)
print(cv_miss)
```


I would choose k_nn = 1 model based on training misclassification rate and would choose k_nn = 2 model based on CV misclassification rate. In practice I will choose the one that minimizes the CV misclassification rate because apparently, if we only choose 1 neighbor, the prediction is just the data itself. Although it has high training accuracy, it doesn't generalize well. Yet the model that has the least CV misclassification has a pretty good training accuracy as well as good generalization. The reason behind this is that, for each fold of the data, we train a model based on all other folds except one left for testing. Once we got the misclassification rate of the test set for this model, do the same thing for all the folds one by one. Compute the average misclassification rate of these models and report that. This rate is more like a test rate rather than training rate, so it indicates both the accuracy and the generalization ability of the model.

# Tutorial for my_rf_cv

```{r}
# Generate a data frame containing k values and corresponding mse 
# in 30 iterations
k_list <- c(2, 5, 10)
itr <- 30
df_total <- data.frame()
mean_cv <- c()
sd_cv <- c()

for (k in k_list) {
  mse_list <- c()
  for (i in 1:itr) {
    mse <- my_rf_cv(k)
    df_curr <- data.frame("k" = k, "mse" = mse)
    df_total <- rbind(df_total, df_curr)
    mse_list <- c(mse_list, mse)
  }
  mean_cv <- c(mean_cv, mean(mse_list))
  sd_cv <- c(sd_cv, sd(mse_list))
}

result <- matrix(c(mean_cv, sd_cv), nrow = 3, ncol = 2, byrow = FALSE)
rownames(result) <- c("k = 2", "k = 5", "k = 10")
colnames(result) <- c("mean_cv", "sd_cv")
as.table(result)

result <- data.frame("mean_cv" = mean_cv, "sd_cv" = sd_cv)
row.names(result) <- c("k = 2", "k = 5", "k = 10")
kable(result, format = "markdown")
```


```{r, fig.width=6}
# Generate a boxplot
df_total %>% 
  ggplot(aes(x = k, y = mse, group = k)) +
  geom_boxplot(fill = "lightblue") +
  labs(title = "CV estimated MSE for different k",
       x = "k",
       y = "CV estimated MSE") +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"),
        plot.caption = element_text(hjust = 0, face = "italic"))
  
```

From the boxplot we can see that for smaller k, the MSE is generally larger. The means from the table also proves our observation. I think this is because, when k is larger, the number of data in the training set is larger. The more data we have for training purpose, the more accurate the model will be, which leads to lower MSE. The standard deviation also decreases as k gets larger, I think it is due to the same reason. When our model is accurate, it has better performance overall. When the model is not that accurate, on the other hand, the prediction somehow depends on luck so the result varies a lot.


























